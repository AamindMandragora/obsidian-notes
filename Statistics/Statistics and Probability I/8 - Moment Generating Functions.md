The $r^\text{th}$ **moment** of a random variable $X$ is the expected value of the $r^\text{th}$ power of $X$, denoted $E[X^r]$, and is also called the moment about the origin or the raw moment. The $r^\text{th}$ **central moment** of $X$ is the expected value of the $r^\text{th}$ power of the deviation of a random variable from its mean, and is denoted $E[(X-\mu_X)^r]$. Therefore, the zeroth moment is the total probability (since $X^0$ is a random variable that outputs only zeroes), the first moment is the expected value, and the second central moment is the variance. The third and fourth moments are skewness and kurtosis, respectively.

Let $X$ be a discrete random variable with pmf $f(x)$ and space $S$. If there is an interval $(-h, h)$ for some real $h$ such that within it, $E[e^{tX}]=\sum_{x\in S}e^{tx}f(x)$ exists and is finite, then the function defined by $M(t)=E[e^{tX}]$ is called the **moment-generating function** of $X$ (or its distribution), often abbreviated as mgf. If two random variables have the same mgf, then they have the same distribution, and for any number of independent random variables, the mgf of their sum is the product of their mgfs. The $r^\text{th}$ derivative of $M_X(t)$ evaluated at $t=0$ is the $r^\text{th}$ moment $E[X^r]$, which means that $M_X(0)=1$ for all distributions.